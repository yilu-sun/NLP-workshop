{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb9c5e2-2d03-4882-bde2-a9af533b9261",
   "metadata": {},
   "source": [
    "### NLP Workshop Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba28f56-103c-4e2e-b1ef-45f8c59c3a42",
   "metadata": {},
   "source": [
    "#### Goals:\n",
    "\n",
    "The goal of this workshop is to showcase how to use different NLP techniques to conduct verbal analytics in dialogue conversations. There are 3 parts altogether:  \n",
    " - Part 1: Improve Dialogue Acts \n",
    " - Part 2: Topic Modeling via LDA\n",
    " - Part 3: Unsupervised Transformers   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f90b16-5f95-4972-87e2-226c8771b0fd",
   "metadata": {},
   "source": [
    "#### Part 1: Improve Dialogue Acts:\n",
    "\n",
    "- The existing Dialogue Act [DialogTag](https://pypi.org/project/DialogTag/) package uses either `bert-base-uncased`, or `distilbert-base-uncased` models, both of which are good at learning word-level associations. This is due to the fact that these networks are trained on the Masked-Language-Modeling objective, wherein, during training, a random word in the input is masked and the network has to learn to predict what this masked word is.\n",
    "    - __Idea__: Instead of using Bert/Distilbert, can we train and use a transformer architecture that learns semantics at a sentence-level? To this point, there exists [Sentence Transformers in Hugging Face](https://huggingface.co/sentence-transformers) that can be trained and then used for inference.\n",
    "        - We can consider finetuning a sentence transformer on the same dataset that the DialogTag authors used to train Bert/Distilbert. The dataset used is the [SwitchBoard Corpus](https://catalog.ldc.upenn.edu/LDC97S62) which is available for download. Following the finetuning procedure, we can use the trained transformer to predict dialogue tags by learning semantics at a sentence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac65aae-06aa-4264-896f-c09e7618c7b0",
   "metadata": {},
   "source": [
    "#### Code Demo: DialogTag improvement: Examples to showcase HuggingFace sentence-transformer usage for a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054acd4-8499-4cb9-845a-9d5bc4b3229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers\n",
    "\n",
    "# Comment: If run into compatibility issue, please install versions: datasets == 2.4.0 transformers == 4.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac207f-4a44-4f34-8f4e-527338583217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef419b-7368-4624-a9bd-5766fbb9117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training,\n",
    "# evaluating, and analyzing natural language understanding systems.\n",
    "\n",
    "# COLA:  Each example is a sequence of words annotated with whether it is a grammatical English sentence.\n",
    "\n",
    "actual_task = \"cola\"\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8729bc3-8127-4099-ba29-8951d5ebfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ec3b7-e817-41bc-a6ee-f10b6cf3d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e5eec-a199-446e-bed3-f4ec2baf443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7eb5fa-edd2-408e-b197-666c3aee1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb31f6-7eb6-4a18-9ba7-9f10e211a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# MiniLM: Small and Fast Pre-trained Models for Language Understanding and Generation\n",
    "model_checkpoint = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Other models that can be explored: https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f9120-69e3-4a47-bcc9-42ec088d1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd9e81-8fd7-44d2-a8ed-61dd99e6896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "}\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[actual_task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22209c-dc39-49d9-b6c7-915901a48b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077709b-3306-4f9a-a9cb-5a5b9f58a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function(dataset['train'][:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65840bb-f313-4581-bf3b-95f1998fbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab6a6e-3e9e-4ce3-b6cc-e7ab12ff40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bolier plate code!\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96f768-6876-4324-a1b1-73ae7e5462d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"matthews_correlation\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "\n",
    "# Following parameters are recommended parameters but could also be tweaked.\n",
    "\n",
    "# Current batch size is set to 16 for demonstrative purposes. If GPU is being used for training, batch size can\n",
    "# take larger values, i.e., 32 / 64 / 128\n",
    "\n",
    "# Weight decay recommended range: [1e-2, 1e-4]\n",
    "# Learning rate: [5e-4, 5e-5]\n",
    "#     Question: (Why do we use such a low learning rate?)\n",
    "\n",
    "# Number of epochs is set to 1, but higher values, in range [5, 10] can be experimented with.\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{actual_task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075b278-7253-4b04-b173-a2add50db2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if actual_task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954c80d-71e3-4264-8655-738b39943d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate code\n",
    "\n",
    "validation_key = \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df377a-566e-47ad-93d2-b3c4aeb50386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell runs for approximately 5 mins. It can take longer if number of epochs is set to \n",
    "# a higher value.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae19e97-5c2f-4f8e-b2cf-e59349dc683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc79ae-fd74-4441-b693-aae5c84b2db3",
   "metadata": {},
   "source": [
    "#### Hyperparam search via Optuna / Ray\\[Tune\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1755b4-6d48-4d7a-a20a-acb40de3d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Trainer supports hyperparameter search using optuna or Ray Tune.\n",
    "# For this last section you will need either of those libraries installed,\n",
    "# just uncomment the line you want on the next cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb36c44-fcc2-4ac7-86a5-cadbe73f466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna\n",
    "# pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646b71d-9f08-4b6b-8db0-2368644c91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate code\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a45bd-5fc6-4f61-aa22-865edffead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223004d-f297-43ec-bf61-eeb36331799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method we call this time is hyperparameter_search. Note that it can take a long time to run on the \n",
    "# full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the \n",
    "# training dataset by replacing the train_dataset line above by:\n",
    "\n",
    "train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10)\n",
    "\n",
    "# for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search.\n",
    "\n",
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
    "\n",
    "# The best run is a config of hyperparameters that achieved the best metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3d9c8-5779-40d6-8ad4-6fb794db614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameter_search method returns a BestRun objects, which contains the value of the \n",
    "# objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run.\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1124a62-2dd7-47cb-92c5-afc53d53c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize the objective to maximize by passing along a compute_objective function to the\n",
    "# hyperparameter_search method, and you can customize the search space by passing a hp_space argument\n",
    "# to hyperparameter_search. See this forum post for some examples.\n",
    "\n",
    "# To reproduce the best training, just set the hyperparameters in your TrainingArgument \n",
    "# before creating a Trainer:\n",
    "\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
