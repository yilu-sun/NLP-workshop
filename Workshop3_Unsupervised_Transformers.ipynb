{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed682310-052f-4ef4-8c1e-7ae1ece82341",
   "metadata": {},
   "source": [
    "### NLP Workshop Part 3\n",
    "\n",
    "#### Unsupervised LM\n",
    "\n",
    "- 3.1: Use pre-trained network to get embeddings, and then run clustering algorithm\n",
    "\n",
    "- 3.2: Finetune a pre-trained network on relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a624e-8488-4472-b5c9-e85a45a704b9",
   "metadata": {},
   "source": [
    "#### Section 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fba761-5cd4-4653-a701-6b55f401818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782d76a-f06c-4048-9403-5225aed69d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-small\")\n",
    "model = AutoModel.from_pretrained(\"johngiorgi/declutr-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4a2cd-626a-4311-a33c-7e8fd68ba2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n",
    "\n",
    "X_train, y_train, y_train_names = newsgroups_train['data'], newsgroups_train['target'], newsgroups_train['target_names'] \n",
    "X_test, y_test, y_test_names = newsgroups_test['data'], newsgroups_test['target'], newsgroups_test['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704eb80a-97f3-48de-8e70-8abf7d146be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fcb41-30f2-4155-9d05-6ce13d1a0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b687f-513c-483f-b33d-2098f2045c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sequence_output = model(**inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93489253-dd0b-4d88-b6d3-bea66d37af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pool the token-level embeddings to get sentence-level embeddings\n",
    "embeddings = torch.sum(\n",
    "    sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n",
    "\n",
    "# embeddings shape is [num_samples, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94024cd2-a57b-4131-9ade-0b1948873f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(embeddings)\n",
    "# Get label assignment (centroid assignment) from KMeans\n",
    "print(\"Labels:\", kmeans.labels_)\n",
    "# Get the cluster centers\n",
    "print(\"Cluster centers:\", kmeans.cluster_centers_)\n",
    "# Infer label for other data based on its proximity to a certain cluster\n",
    "print(kmeans.predict(embeddings[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd305de-623d-4fa6-bdad-88b7596d4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\n",
    "\n",
    "# Tokenizer gives HTTP error:\n",
    "from cocolm.modeling_cocolm import COCOLMModel\n",
    "from cocolm.configuration_cocolm import COCOLMConfig\n",
    "from cocolm.tokenization_cocolm import COCOLMTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713f965-cbf2-41af-a96b-5d7ebde6e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base, large = max seq length is 512\n",
    "model = \"microsoft/cocolm-base\"\n",
    "config = COCOLMConfig.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea049ea3-0dfe-463a-9105-a09147ae47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b413d-d80f-4279-aca0-0264f58b20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = COCOLMModel.from_pretrained(model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d663e3-770c-44bc-852b-5069e78517ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = COCOLMTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149eaa83-4e53-427e-a6b0-635a6ff278aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "\n",
    "for x in data:\n",
    "    encoded = tokenizer.encode(x)\n",
    "    if len(encoded) <= 512:\n",
    "        inputs.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b147c4e-7796-4b46-a6ad-b64e4eb84ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for x in inputs:\n",
    "        embedding = model(torch.tensor([x]))[0] # 1 x 192 x 768\n",
    "        embedding = torch.mean(embedding[0], axis=0)\n",
    "        outputs.append(embedding)\n",
    "outputs = torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302d2df-972c-4706-990f-dd806e1f03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(outputs)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.predict(outputs[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bc6da-4ff1-450a-bce4-b64604d1df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPNet\n",
    "\n",
    "from transformers import MPNetTokenizer, MPNetModel\n",
    "\n",
    "tokenizer = MPNetTokenizer.from_pretrained('microsoft/mpnet-base')\n",
    "model = MPNetModel.from_pretrained('microsoft/mpnet-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fe2cb-a586-49f7-b050-6a19b237062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f777ca0-bdd1-41a0-8e2b-9ab8d9220699",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697109e-0f00-4afa-a80e-1b49c0b85e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state # N x 512 x 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfcfe0-8de2-4001-9c44-23dac9c0e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = torch.mean(last_hidden_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3d629-40e2-4f8f-bb5c-12c94c0dfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d05cb-f54d-4268-9c85-1459add7d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(last_hidden_states)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.predict(last_hidden_states[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ca7e-b914-4db8-a6af-e866749bad07",
   "metadata": {},
   "source": [
    "#### Section 3.2: Train sentence transformer:\n",
    "\n",
    "#### How?\n",
    "\n",
    "We will need to know which pairs are closer and which ones are far.\n",
    "\n",
    "Given that we don't have labels, we will have to use some heuristic for this.\n",
    "For instance, our dataset will contain a mapping between BQs being evaluated and the prompt and the response.\n",
    "\n",
    "As a result, we can try to bring responses corresponding to the same prompt closer in the embedding space.\n",
    "Responses from different prompts should lie further apart in the embedding space.\n",
    "\n",
    "__Some discussion needs to happen re: label while training the model. Prompts belonging to the same BQ can have\n",
    "higher similarity scores?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e5931-75cc-4291-adc3-cf9cb43135dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers import evaluation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba06aea-24e5-4391-b02e-0967a1544135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens') # more models : https://www.sbert.net/docs/training/overview.html\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb0e96-4d97-46da-8c5c-1c2c4739cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.CosineSimilarityLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499e720-6433-4429-bf38-34f6f88e732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']\n",
    "sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
    "scores = [0.3, 0.6, 0.2]\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42e7ce-a7fd-42fc-b0cd-be78da2dd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=2,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=5,\n",
    "    output_path=\"./sentence_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c5658-b107-4969-8155-3b0ac9ba65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model = SentenceTransformer('./sentence_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372a816-1cca-4d3a-9b56-a427e0fc0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BERT on custom MLM:\n",
    "\n",
    "# https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020b0b6-8b0f-4e6b-a310-fe20e4ccda13",
   "metadata": {},
   "source": [
    "#### Other Verbal Analytics Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02585804-8d31-4cd3-88b8-67c4aeb8c892",
   "metadata": {},
   "source": [
    "There are many other different verbal analytics. Sharing some useful links below for further exploration: \n",
    "\n",
    "- __Semantic Matching__       \n",
    "    - __Idea 1__: We could explore using transformers that capture sentence-level semantics. In other words, using networks that are pre-trained on tasks such as, question-answering, paraphrasing and summarization. To this point, the following transformer architectures have shown SOTA performance on the above tasks:\n",
    "        * [paraphrase-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2)\n",
    "        * [MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)\n",
    "        * [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "        * Useful resource: HuggingFace's official page on [Semantic Matching](https://huggingface.co/tasks/sentence-similarity)\n",
    "          \n",
    "    - __Idea 2__: While we are using sentence transformers to compare 2 sentences semantically, we can also compare 2 sentences syntactically. There exists several traditional ML-based ideas that compare 2 sentences based on their syntax, such as:\n",
    "        * TF-IDF [What is it?](https://monkeylearn.com/blog/what-is-tf-idf/)  |  [Sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "        * Count Vectorizer [What is it?](https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c) | [Sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "        * FuzzyWuzzy [String matching Python package](https://pypi.org/project/fuzzywuzzy/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
